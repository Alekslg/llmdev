---
title: "Chat models"
topic: "Concepts"
filename: "chat-models.md"
source: "https://python.langchain.com/docs/concepts/chat_models"
author: "Перевод GPT"
date: "2025-09-11"
---

# Чат-модели

## Обзор

Large Language Models (LLMs) — это продвинутые модели машинного обучения, которые превосходно справляются с широким спектром задач, связанных с языком, таких как генерация текста, перевод, суммирование, ответ на вопросы и многое другое, без необходимости тонкой настройки под каждую задачу.

Современные LLM обычно используются через интерфейс **chat model**, который принимает список сообщений (messages) в качестве входных данных и возвращает одно сообщение (message) как выход. :contentReference[oaicite:0]{index=0}

Новейшее поколение chat моделей предлагает дополнительные возможности:

- **Tool calling**: многие популярные chat модели предоставляют нативный API для вызова инструментов. Этот API позволяет разработчикам строить богатые приложения, в которых LLM могут взаимодействовать с внешними сервисами, API и базами данных. Вызов инструментов также может использоваться для извлечения структурированной информации из неструктурированных данных и выполнения различных других задач. :contentReference[oaicite:1]{index=1}
- **Structured output**: техника, позволяющая требовать от chat модели ответ в структурированном формате, например JSON, соответствующем заданной схеме. :contentReference[oaicite:2]{index=2}
- **Multimodality**: способность работать с данными, отличными от текста; например, изображения, аудио и видео. :contentReference[oaicite:3]{index=3}

## Возможности

LangChain предоставляет единообразный интерфейс для работы с chat моделями разных провайдеров, а также дополнительные функции для мониторинга, отладки и оптимизации производительности приложений, использующих LLM. :contentReference[oaicite:4]{index=4}

- Интеграции со многими провайдерами chat моделей (например, Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). См. [chat model integrations](https://python.langchain.com/docs/concepts/chat_models#integrations) для актуального списка поддерживаемых моделей. :contentReference[oaicite:5]{index=5}
- Возможность использовать либо формат сообщений LangChain (messages) либо формат сообщений OpenAI. :contentReference[oaicite:6]{index=6}
- Стандартный API для **tool calling**: стандартный интерфейс для привязки инструментов к моделям, для доступа к запросам на вызов инструмента, сделанным моделями, и для передачи обратно результатов работы инструментов модели. :contentReference[oaicite:7]{index=7}
- Стандартный API для **structuring outputs** через метод `with_structured_output`. :contentReference[oaicite:8]{index=8}
- Поддержка **async programming**, **efficient batching**, **rich streaming API**. :contentReference[oaicite:9]{index=9}
- Интеграция с LangSmith для мониторинга и отладки приложений, использующих LLM в продакшене. :contentReference[oaicite:10]{index=10}
- Дополнительные функции: стандартизированное отслеживание использования токенов (token usage), ограничение скорости запросов (rate limiting), кеширование (caching) и другие. :contentReference[oaicite:11]{index=11}

## Интеграции

LangChain имеет множество интеграций с chat моделями, которые позволяют использовать разнообразие моделей от разных провайдеров. :contentReference[oaicite:12]{index=12}

Эти интеграции бывают двух типов:

1. **Official models**: Это модели, которые официально поддерживаются LangChain и/или провайдером модели. Вы можете найти эти модели в пакетах `langchain-<provider>`. :contentReference[oaicite:13]{index=13}
2. **Community models**: Модели, в основном внесённые и поддерживаемые сообществом. Вы можете найти эти модели в пакете `langchain-community`. :contentReference[oaicite:14]{index=14}

Модели chat в LangChain именуются соглашением, которое добавляет префикс "Chat" к их именам классов (например, `ChatOllama`, `ChatAnthropic`, `ChatOpenAI` и т.д.). :contentReference[oaicite:15]{index=15}

> Примечание: Модели, которые не имеют префикса "Chat" в своём имени или содержат суффикс "LLM", как правило, относятся к старым моделям, которые не используют интерфейс chat model, а вместо этого используют интерфейс, принимающий строку (string) как вход и возвращающий строку как выход. :contentReference[oaicite:16]{index=16}

## Интерфейс

Chat модели в LangChain реализуют интерфейс **BaseChatModel**. Поскольку `BaseChatModel` также реализует **Runnable Interface**, chat модели поддерживают стандартный streaming интерфейс, **async programming**, оптимизированную батчинг (efficient batching) и другие возможности. :contentReference[oaicite:17]{index=17}

Многие ключевые методы chat моделей работают с входом в виде сообщений (messages) и возвращают сообщения как ответ. :contentReference[oaicite:18]{index=18}

## Основные методы

Ключевые методы chat модели:

1. `invoke`: основной метод для взаимодействия с chat моделью. Принимает список сообщений (messages) как вход и возвращает список сообщений как выход. :contentReference[oaicite:19]{index=19}
2. `stream`: метод, который позволяет получать выход chat модели по мере его генерации. :contentReference[oaicite:20]{index=20}
3. `batch`: метод, который позволяет группировать несколько запросов к chat модели вместе для более эффективной обработки. :contentReference[oaicite:21]{index=21}
4. `bind_tools`: метод, позволяющий связать инструмент с chat моделью для использования в контексте выполнения модели. :contentReference[oaicite:22]{index=22}
5. `with_structured_output`: оболочка вокруг метода `invoke` для моделей, которые нативно поддерживают **structured output**. :contentReference[oaicite:23]{index=23}

Другие важные методы см. в [BaseChatModel API Reference](https://python.langchain.com/docs/concepts/chat_models#interface). :contentReference[oaicite:24]{index=24}

## Входы и выходы

Современные LLM обычно используются через интерфейс chat model, который принимает `messages` как вход и возвращает `messages` как выход. Сообщения обычно связаны с ролью (например, "system", "human", "assistant") и с одним или несколькими блоками содержимого (content), которые содержат текст или потенциально мультимодальные данные (например, изображения, аудио, видео). :contentReference[oaicite:25]{index=25}

LangChain поддерживает два формата сообщений для взаимодействия с chat моделями:

1. **LangChain Message Format**: собственный формат сообщений LangChain, используется по умолчанию и внутри LangChain. :contentReference[oaicite:26]{index=26}
2. **OpenAI's Message Format**: формат сообщений OpenAI. :contentReference[oaicite:27]{index=27}

## Стандартные параметры

Многие chat модели имеют стандартизированные параметры, которые могут использоваться для настройки поведения модели:

| Параметр       | Описание                                                                                                                                                                                                                                         |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `model`        | Название или идентификатор конкретной AI модели, которую вы хотите использовать (например, `"gpt-3.5-turbo"` или `"gpt-4"`). :contentReference[oaicite:28]{index=28}                                                                             |
| `temperature`  | Управляет случайностью вывода модели. Более высокое значение (например, 1.0) делает ответы более креативными, чем низкое (например, 0.0), которое делает их более детерминированными и сфокусированными. :contentReference[oaicite:29]{index=29} |
| `timeout`      | Максимальное время (в секундах), чтобы ждать ответ от модели до отмены запроса. Гарантирует, что запрос не зависнет бесконечно. :contentReference[oaicite:30]{index=30}                                                                          |
| `max_tokens`   | Ограничивает общее число токенов (слов и пунктуации) в ответе. Управляет тем, насколько длинным может быть выход. :contentReference[oaicite:31]{index=31}                                                                                        |
| `stop`         | Задаёт последовательности остановки, которые указывают, когда модель должна прекратить генерировать токены. Например, вы можете использовать специфические строки для обозначения конца ответа. :contentReference[oaicite:32]{index=32}          |
| `max_retries`  | Максимальное количество попыток, которые система предпримет, чтобы отправить запрос повторно, если он не удался из-за проблем вроде сетевых таймаутов или превышения лимита скорости. :contentReference[oaicite:33]{index=33}                    |
| `api_key`      | API ключ, необходимый для аутентификации у провайдера модели. Обычно выдается при регистрации. :contentReference[oaicite:34]{index=34}                                                                                                           |
| `base_url`     | URL API конечной точки, куда отправляются запросы. Обычно предоставляется провайдером модели и необходим для направления ваших запросов. :contentReference[oaicite:35]{index=35}                                                                 |
| `rate_limiter` | Необязательный параметр типа `BaseRateLimiter`, чтобы распределять запросы, избегая превышения лимитов. См. раздел про rate limiting для деталей. :contentReference[oaicite:36]{index=36}                                                        |

Некоторые важные замечания:

- Стандартные параметры применимы только к тем провайдерам моделей, которые открывают параметры с соответствующей функциональностью. Например, некоторые провайдеры не дают настройки максимального числа токенов в ответе, так что `max_tokens` может быть не поддержан. :contentReference[oaicite:37]{index=37}
- Стандартные параметры в настоящее время применяются только к интеграциям, у которых есть собственные пакеты интеграции (например, `langchain-openai`, `langchain-anthropic` и т.д.), они не принудительно поддерживаются моделями в `langchain-community`. :contentReference[oaicite:38]{index=38}

Chat модели также принимают другие параметры, специфичные для данной интеграции. Чтобы узнать все параметры, поддерживаемые определённой chat моделью, обратитесь к их соответствующему [API Reference](https://python.langchain.com/docs/concepts/chat_models#interface) для этой модели. :contentReference[oaicite:39]{index=39}

## Вызов инструментов (Tool calling)

Chat модели могут вызывать **tools** для выполнения задач, таких как получение данных из базы данных, выполнение API-запросов или запуск пользовательского кода. Более подробную информацию см. в руководстве по [tool calling](https://python.langchain.com/docs/concepts/chat_models#tool-calling). :contentReference[oaicite:40]{index=40}

## Структурированные ответы (Structured outputs)

Chat модели могут быть запрошены выдать ответ в определённом формате (например, JSON или соответствующем определённой схеме). Эта возможность крайне полезна для задач по извлечению информации. Подробнее см. в руководстве [structured outputs](https://python.langchain.com/docs/concepts/chat_models#structured-outputs). :contentReference[oaicite:41]{index=41}

## Мультимодальность (Multimodality)

Large Language Models (LLMs) не ограничены обработкой только текста. Они также могут использоваться для обработки других типов данных, таких как изображения, аудио и видео. Это называется multimodality. :contentReference[oaicite:42]{index=42}

В настоящий момент лишь некоторые LLM поддерживают мультимодальные входы, и почти ни одна — мультимодальные выходы. Пожалуйста, проверьте документацию конкретной модели на предмет деталей. :contentReference[oaicite:43]{index=43}

## Контекстное окно (Context window)

Контекстное окно chat модели — это максимальный размер входной последовательности, которую модель может обработать за один раз. Несмотря на то, что контекстные окна современных LLM довольно велики, они всё же представляют собой ограничение, о котором разработчикам необходимо помнить при работе с chat моделями. :contentReference[oaicite:44]{index=44}

Если вход превышает размер контекстного окна, модель может быть неспособна обработать весь ввод и может выдать ошибку. В разговорных приложениях это особенно важно, потому что контекстное окно определяет, сколько информации модель "запоминает" в ходе диалога. Разработчикам часто нужно управлять вводом так, чтобы он оставался в пределах контекстного окна, чтобы сохранять связный диалог без превышения лимита. :contentReference[oaicite:45]{index=45}

Размер входа измеряется в **tokens** — единицах обработки, которые модель использует. :contentReference[oaicite:46]{index=46}

## Продвинутые темы

### Ограничение скорости запросов (Rate-limiting)

Многие провайдеры chat моделей вводят ограничение на число запросов, которые можно сделать в течение определённого периода времени.

Если вы столкнулись с ограничением скорости (rate limit), вы обычно получите ошибку от провайдера, и вам придётся подождать, прежде чем делать новые запросы.

У вас есть несколько вариантов, как справиться с ограничениями скорости:

1. Старайтесь избегать превышения лимитов, распределяя запросы: chat модели принимают параметр `rate_limiter`, который можно передать при инициализации. Этот параметр используется для регулирования того, с какой частотой отправляются запросы к провайдеру модели. Распределение запросов особенно полезно при benchmarking моделей для оценки их производительности. Подробнее см. [how to handle rate limits](https://python.langchain.com/docs/concepts/chat_models#advanced-topics) для подробной информации. :contentReference[oaicite:47]{index=47}
2. Пытаться восстанавливаться после ошибок превышения лимита: если вы получаете ошибку rate limit, можно подождать некоторое время перед повторной попыткой запроса. Время ожидания можно увеличивать с каждой последующей ошибкой. Chat модели имеют параметр `max_retries`, который можно использовать для контроля числа таких попыток. Подробнее в разделе стандартных параметров. :contentReference[oaicite:48]{index=48}
3. Переключаться на другую chat модель: если с одной моделью вы столкнулись с превышением лимита, можно перейти на другую модель, у которой лимиты менее строгие или иные условия. :contentReference[oaicite:49]{index=49}

### Кеширование (Caching)

API chat моделей могут работать медленно, поэтому естественный вопрос: стоит ли кешировать результаты предыдущих разговоров. Теоретически кеширование может помочь повысить производительность за счёт уменьшения числа запросов к провайдеру модели. На практике кеширование ответов chat модели — сложная задача, и к ней нужно подходить с осторожностью. :contentReference[oaicite:50]{index=50}

Причина в том, что шанс попадания в кеш мал после первого-второго взаимодействия в разговоре, если опираться на кеширование точных входов модели. Например: какова вероятность, что несколько разговоров начнутся с точно того же сообщения? А что насчёт трёх одинаковых сообщений подряд? :contentReference[oaicite:51]{index=51}

Альтернативный подход — семантическое кеширование (semantic caching), когда вы кешируете ответы на основе смысла ввода, а не точного ввода. Это может быть эффективным в некоторых ситуациях, но не во всех. :contentReference[oaicite:52]{index=52}

Семантический кеш вводит зависимость от другой модели на критическом пути приложения (например, семантический кеш может использовать embedding model для преобразования текста в вектор). И нет гарантии, что он точно уловит смысл ввода. :contentReference[oaicite:53]{index=53}

Однако могут быть ситуации, когда кеширование ответов chat модели выгодно. Например, если у вас есть chat модель, используемая для ответов на часто задаваемые вопросы, кеширование ответов может помочь уменьшить нагрузку на провайдера модели, сократить издержки и улучшить время отклика. :contentReference[oaicite:54]{index=54}

См. руководство [how to cache chat model responses](https://python.langchain.com/docs/concepts/chat_models#advanced-topics) для более детальной информации. :contentReference[oaicite:55]{index=55}

## Связанные ресурсы

- Руководства-How-to по использованию chat моделей: [how-to guides](https://python.langchain.com/docs/concepts/chat_models#related-resources) :contentReference[oaicite:56]{index=56}
- Список поддерживаемых chat моделей: [chat model integrations](https://python.langchain.com/docs/concepts/chat_models#integrations) :contentReference[oaicite:57]{index=57}

### Концептуальные руководства

- [Messages](https://python.langchain.com/docs/concepts/chat_models#inputs-and-outputs) :contentReference[oaicite:58]{index=58}
- [Tool calling](https://python.langchain.com/docs/concepts/chat_models#tool-calling) :contentReference[oaicite:59]{index=59}
- [Multimodality](https://python.langchain.com/docs/concepts/chat_models#multimodality) :contentReference[oaicite:60]{index=60}
- [Structured outputs](https://python.langchain.com/docs/concepts/chat_models#structured-outputs) :contentReference[oaicite:61]{index=61}
- [Tokens](https://python.langchain.com/docs/concepts/chat_models#context-window) :contentReference[oaicite:62]{index=62}

Source: https://python.langchain.com/docs/concepts/chat_models

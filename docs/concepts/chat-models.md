---
title: "Chat models"
topic: "Concepts"
filename: "chat-models.md"
source: "https://python.langchain.com/docs/concepts/chat_models"
author: "Перевод GPT"
date: "2025-09-11"
---

# Чат-модели

## Обзор

Large Language Models (LLMs) — это продвинутые модели машинного обучения, которые превосходно справляются с широким спектром задач, связанных с языком, таких как генерация текста, перевод, суммирование, ответ на вопросы и многое другое, без необходимости тонкой настройки под каждую задачу.

Современные LLM обычно используются через интерфейс **chat model**, который принимает список сообщений (messages) в качестве входных данных и возвращает одно сообщение (message) как выход.

Новейшее поколение chat моделей предлагает дополнительные возможности:

- **Tool calling**: многие популярные chat модели предоставляют нативный API для вызова инструментов. Этот API позволяет разработчикам строить богатые приложения, в которых LLM могут взаимодействовать с внешними сервисами, API и базами данных. Вызов инструментов также может использоваться для извлечения структурированной информации из неструктурированных данных и выполнения различных других задач.
- **Structured output**: техника, позволяющая требовать от chat модели ответ в структурированном формате, например JSON, соответствующем заданной схеме.
- **Multimodality**: способность работать с данными, отличными от текста; например, изображения, аудио и видео.

## Возможности

LangChain предоставляет единообразный интерфейс для работы с chat моделями разных провайдеров, а также дополнительные функции для мониторинга, отладки и оптимизации производительности приложений, использующих LLM.

- Интеграции со многими провайдерами chat моделей (например, Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). См. [chat model integrations](https://python.langchain.com/docs/concepts/chat_models#integrations) для актуального списка поддерживаемых моделей.
- Возможность использовать либо формат сообщений LangChain (messages) либо формат сообщений OpenAI.
- Стандартный API для **tool calling**: стандартный интерфейс для привязки инструментов к моделям, для доступа к запросам на вызов инструмента, сделанным моделями, и для передачи обратно результатов работы инструментов модели.
- Стандартный API для **structuring outputs** через метод `with_structured_output`.
- Поддержка **async programming**, **efficient batching**, **rich streaming API**.
- Интеграция с LangSmith для мониторинга и отладки приложений, использующих LLM в продакшене.
- Дополнительные функции: стандартизированное отслеживание использования токенов (token usage), ограничение скорости запросов (rate limiting), кеширование (caching) и другие.

## Интеграции

LangChain имеет множество интеграций с chat моделями, которые позволяют использовать разнообразие моделей от разных провайдеров.

Эти интеграции бывают двух типов:

1. **Official models**: Это модели, которые официально поддерживаются LangChain и/или провайдером модели. Вы можете найти эти модели в пакетах `langchain-<provider>`.
2. **Community models**: Модели, в основном внесённые и поддерживаемые сообществом. Вы можете найти эти модели в пакете `langchain-community`.

Модели chat в LangChain именуются соглашением, которое добавляет префикс "Chat" к их именам классов (например, `ChatOllama`, `ChatAnthropic`, `ChatOpenAI` и т.д.).

> Примечание: Модели, которые не имеют префикса "Chat" в своём имени или содержат суффикс "LLM", как правило, относятся к старым моделям, которые не используют интерфейс chat model, а вместо этого используют интерфейс, принимающий строку (string) как вход и возвращающий строку как выход.

## Интерфейс

Chat модели в LangChain реализуют интерфейс **BaseChatModel**. Поскольку `BaseChatModel` также реализует **Runnable Interface**, chat модели поддерживают стандартный streaming интерфейс, **async programming**, оптимизированную батчинг (efficient batching) и другие возможности.

Многие ключевые методы chat моделей работают с входом в виде сообщений (messages) и возвращают сообщения как ответ.

## Основные методы

Ключевые методы chat модели:

1. `invoke`: основной метод для взаимодействия с chat моделью. Принимает список сообщений (messages) как вход и возвращает список сообщений как выход.
2. `stream`: метод, который позволяет получать выход chat модели по мере его генерации.
3. `batch`: метод, который позволяет группировать несколько запросов к chat модели вместе для более эффективной обработки.
4. `bind_tools`: метод, позволяющий связать инструмент с chat моделью для использования в контексте выполнения модели.
5. `with_structured_output`: оболочка вокруг метода `invoke` для моделей, которые нативно поддерживают **structured output**.

Другие важные методы см. в [BaseChatModel API Reference](https://python.langchain.com/docs/concepts/chat_models#interface).

## Входы и выходы

Современные LLM обычно используются через интерфейс chat model, который принимает `messages` как вход и возвращает `messages` как выход. Сообщения обычно связаны с ролью (например, "system", "human", "assistant") и с одним или несколькими блоками содержимого (content), которые содержат текст или потенциально мультимодальные данные (например, изображения, аудио, видео).

LangChain поддерживает два формата сообщений для взаимодействия с chat моделями:

1. **LangChain Message Format**: собственный формат сообщений LangChain, используется по умолчанию и внутри LangChain.
2. **OpenAI's Message Format**: формат сообщений OpenAI.

## Стандартные параметры

Многие chat модели имеют стандартизированные параметры, которые могут использоваться для настройки поведения модели:

| Параметр       | Описание                                                                                                                                                                                                 |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `model`        | Название или идентификатор конкретной AI модели, которую вы хотите использовать (например, `"gpt-3.5-turbo"` или `"gpt-4"`).                                                                             |
| `temperature`  | Управляет случайностью вывода модели. Более высокое значение (например, 1.0) делает ответы более креативными, чем низкое (например, 0.0), которое делает их более детерминированными и сфокусированными. |
| `timeout`      | Максимальное время (в секундах), чтобы ждать ответ от модели до отмены запроса. Гарантирует, что запрос не зависнет бесконечно.                                                                          |
| `max_tokens`   | Ограничивает общее число токенов (слов и пунктуации) в ответе. Управляет тем, насколько длинным может быть выход.                                                                                        |
| `stop`         | Задаёт последовательности остановки, которые указывают, когда модель должна прекратить генерировать токены. Например, вы можете использовать специфические строки для обозначения конца ответа.          |
| `max_retries`  | Максимальное количество попыток, которые система предпримет, чтобы отправить запрос повторно, если он не удался из-за проблем вроде сетевых таймаутов или превышения лимита скорости.                    |
| `api_key`      | API ключ, необходимый для аутентификации у провайдера модели. Обычно выдается при регистрации.                                                                                                           |
| `base_url`     | URL API конечной точки, куда отправляются запросы. Обычно предоставляется провайдером модели и необходим для направления ваших запросов.                                                                 |
| `rate_limiter` | Необязательный параметр типа `BaseRateLimiter`, чтобы распределять запросы, избегая превышения лимитов. См. раздел про rate limiting для деталей.                                                        |

Некоторые важные замечания:

- Стандартные параметры применимы только к тем провайдерам моделей, которые открывают параметры с соответствующей функциональностью. Например, некоторые провайдеры не дают настройки максимального числа токенов в ответе, так что `max_tokens` может быть не поддержан.
- Стандартные параметры в настоящее время применяются только к интеграциям, у которых есть собственные пакеты интеграции (например, `langchain-openai`, `langchain-anthropic` и т.д.), они не принудительно поддерживаются моделями в `langchain-community`.

Chat модели также принимают другие параметры, специфичные для данной интеграции. Чтобы узнать все параметры, поддерживаемые определённой chat моделью, обратитесь к их соответствующему [API Reference](https://python.langchain.com/docs/concepts/chat_models#interface) для этой модели.

## Вызов инструментов (Tool calling)

Chat модели могут вызывать **tools** для выполнения задач, таких как получение данных из базы данных, выполнение API-запросов или запуск пользовательского кода. Более подробную информацию см. в руководстве по [tool calling](https://python.langchain.com/docs/concepts/chat_models#tool-calling).

## Структурированные ответы (Structured outputs)

Chat модели могут быть запрошены выдать ответ в определённом формате (например, JSON или соответствующем определённой схеме). Эта возможность крайне полезна для задач по извлечению информации. Подробнее см. в руководстве [structured outputs](https://python.langchain.com/docs/concepts/chat_models#structured-outputs).

## Мультимодальность (Multimodality)

Large Language Models (LLMs) не ограничены обработкой только текста. Они также могут использоваться для обработки других типов данных, таких как изображения, аудио и видео. Это называется multimodality.

В настоящий момент лишь некоторые LLM поддерживают мультимодальные входы, и почти ни одна — мультимодальные выходы. Пожалуйста, проверьте документацию конкретной модели на предмет деталей.

## Контекстное окно (Context window)

Контекстное окно chat модели — это максимальный размер входной последовательности, которую модель может обработать за один раз. Несмотря на то, что контекстные окна современных LLM довольно велики, они всё же представляют собой ограничение, о котором разработчикам необходимо помнить при работе с chat моделями.

Если вход превышает размер контекстного окна, модель может быть неспособна обработать весь ввод и может выдать ошибку. В разговорных приложениях это особенно важно, потому что контекстное окно определяет, сколько информации модель "запоминает" в ходе диалога. Разработчикам часто нужно управлять вводом так, чтобы он оставался в пределах контекстного окна, чтобы сохранять связный диалог без превышения лимита.

Размер входа измеряется в **tokens** — единицах обработки, которые модель использует.

## Продвинутые темы

### Ограничение скорости запросов (Rate-limiting)

Многие провайдеры chat моделей вводят ограничение на число запросов, которые можно сделать в течение определённого периода времени.

Если вы столкнулись с ограничением скорости (rate limit), вы обычно получите ошибку от провайдера, и вам придётся подождать, прежде чем делать новые запросы.

У вас есть несколько вариантов, как справиться с ограничениями скорости:

1. Старайтесь избегать превышения лимитов, распределяя запросы: chat модели принимают параметр `rate_limiter`, который можно передать при инициализации. Этот параметр используется для регулирования того, с какой частотой отправляются запросы к провайдеру модели. Распределение запросов особенно полезно при benchmarking моделей для оценки их производительности. Подробнее см. [how to handle rate limits](https://python.langchain.com/docs/concepts/chat_models#advanced-topics) для подробной информации.
2. Пытаться восстанавливаться после ошибок превышения лимита: если вы получаете ошибку rate limit, можно подождать некоторое время перед повторной попыткой запроса. Время ожидания можно увеличивать с каждой последующей ошибкой. Chat модели имеют параметр `max_retries`, который можно использовать для контроля числа таких попыток. Подробнее в разделе стандартных параметров.
3. Переключаться на другую chat модель: если с одной моделью вы столкнулись с превышением лимита, можно перейти на другую модель, у которой лимиты менее строгие или иные условия.

### Кеширование (Caching)

API chat моделей могут работать медленно, поэтому естественный вопрос: стоит ли кешировать результаты предыдущих разговоров. Теоретически кеширование может помочь повысить производительность за счёт уменьшения числа запросов к провайдеру модели. На практике кеширование ответов chat модели — сложная задача, и к ней нужно подходить с осторожностью.

Причина в том, что шанс попадания в кеш мал после первого-второго взаимодействия в разговоре, если опираться на кеширование точных входов модели. Например: какова вероятность, что несколько разговоров начнутся с точно того же сообщения? А что насчёт трёх одинаковых сообщений подряд?

Альтернативный подход — семантическое кеширование (semantic caching), когда вы кешируете ответы на основе смысла ввода, а не точного ввода. Это может быть эффективным в некоторых ситуациях, но не во всех.

Семантический кеш вводит зависимость от другой модели на критическом пути приложения (например, семантический кеш может использовать embedding model для преобразования текста в вектор). И нет гарантии, что он точно уловит смысл ввода.

Однако могут быть ситуации, когда кеширование ответов chat модели выгодно. Например, если у вас есть chat модель, используемая для ответов на часто задаваемые вопросы, кеширование ответов может помочь уменьшить нагрузку на провайдера модели, сократить издержки и улучшить время отклика.

См. руководство [how to cache chat model responses](https://python.langchain.com/docs/concepts/chat_models#advanced-topics) для более детальной информации.

## Связанные ресурсы

- Руководства-How-to по использованию chat моделей: [how-to guides](https://python.langchain.com/docs/concepts/chat_models#related-resources)
- Список поддерживаемых chat моделей: [chat model integrations](https://python.langchain.com/docs/concepts/chat_models#integrations)

### Концептуальные руководства

- [Messages](https://python.langchain.com/docs/concepts/chat_models#inputs-and-outputs)
- [Tool calling](https://python.langchain.com/docs/concepts/chat_models#tool-calling)
- [Multimodality](https://python.langchain.com/docs/concepts/chat_models#multimodality)
- [Structured outputs](https://python.langchain.com/docs/concepts/chat_models#structured-outputs)
- [Tokens](https://python.langchain.com/docs/concepts/chat_models#context-window)

Source: https://python.langchain.com/docs/concepts/chat_models
